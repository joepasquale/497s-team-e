In this project we will discuss how to horizontally scale database systems.
Horizontal scaling means adding more machines to the system to share the workload.
(Vertical scaling means adding more RAM/CPUs/Storage/Network capacity to the same machine, and it has more to do with the hardware update, so it won't be discussed here.)  
The way to scale PostgresSql and MongoDB are not identical so we will discuss them seperately. 
We will also use cache to accelerate database, which will be throughly discussed at Code Explanation section.

# Scale PostgresSql
For PostgresSQL, we can split one database into a master database(for write only) and several slave databases (for read only). 
The slave databases periodically get update from the master database and all the read requests go to them.
This is good method to add both read & write capacity because:
1) The master database does not need to handle read queries, which safes CPU time for inserting new data.
2) The master database can caching more indexing information for insertion in memory.
3) The slave only needs to handle read requests, it can cache a lot of result in memory.
4) Multiple slaves can share the read wordloads which increased the throughput.
The problem with this approach is that writing and reading are asynchronous, client cannot get the latest update when new data is added.
Though, this is not a big problem for most of the function in our app. 

# Scale MongoDB
The data replica sets schema of MongoDB (which is same as the master-slaves model described above) is only for increasing data availability instead of increasing throughput. According to MongoDB: 
> "All members of a replica have roughly equivalent write traffic; as a result, secondaries will service reads at roughtly the same rate as the primary."

For MongoDB, horizontal scaling is accompished by sharding. 
In this case, a database is deployed to a cluster of servers. 
Internally, MongoDB splits data in each collection into several subsets/chunks, and each server in the cluster handles a subset of the data requests. 

For each server, MongoDB keeps a maxKey as the upper bound to the range of Shard Key it shall contains.
If the Shard Key is monotonical increasing, it is very likely that all inserts goes to the same chunk in one server, which makes sharding useless.
To solve this, MongoDB also introduces hashed Shard Key to evenly distribute workload, and it is recommended to use the default ObjectID (which is automatically generated using the timestamp of the data inserted) as the Shard Key. We will follow the recommendation here.
> The field you choose as your hashed shard key should have a good cardinality, or large number of different values. Hashed keys are ideal for shard keys with fields that change monotonically like ObjectId values or timestamps. A good example of this is the default _id field, assuming it only contains ObjectID values. 
Also,
> MongoDB automatically computes the hashes when resolving queries using hashed indexes. Applications do not need to compute hashes.

Simply put, the work need to be done here is choosing the Shard Key for each collection, and this Shard Key is likely the _id which is automatically generated by MongoDB.

## Code Explanation:
We will use a simple register/login service to illustrate how to scale MongoDB in this project.
The backend is written in Go.
Two endpoints are disposed:
/login      ["GET", "POST"] 
Accepting: ["email": "", "password": ""]

/register   ["GET", "POST"] 
Accepting: ["email": "", "password": ""]

When user try to login, the email and password will send to the backend for authentication.
If unsuccessful, user password will be cached in redis for next login.

Database is using MongoDB and also sharded for scaling.
